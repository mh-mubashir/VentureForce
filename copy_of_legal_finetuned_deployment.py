# -*- coding: utf-8 -*-
"""Copy of Legal_FineTuned_Deployment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ny2rLfzDi3ryw8fZFOtdcvi71cpZBNs
"""

# Unsloth Model API Deployment

# Install required packages
!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
!pip install --no-deps unsloth
!pip install fastapi uvicorn pyngrok

# Import libraries
import uvicorn
import threading
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from pyngrok import ngrok
import torch
#from unsloth import FastLanguageModel
from transformers import TextStreamer
from transformers import pipeline
from huggingface_hub import HfFolder, InferenceApi
import torch
from unsloth import FastLanguageModel
from transformers import TextStreamer
from unsloth import FastLanguageModel

#2
# import torch
# from unsloth import FastLanguageModel
# from transformers import TextStreamer

# Load your model locally
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="asadwaraich/legal-advice-llama",
    max_seq_length=10000,
    dtype=None,
    load_in_4bit=True,
    device_map="auto"  # This will automatically manage GPU and CPU memory
)
FastLanguageModel.for_inference(model)

import torch
import numpy as np
from fastapi import FastAPI
from pydantic import BaseModel
from pyngrok import ngrok
from threading import Thread
from transformers import AutoTokenizer, AutoModelForCausalLM
import nest_asyncio
import json

app = FastAPI()
class ProjectRequest(BaseModel):
    instruction: str
    input_text: str = ""
    max_new_tokens: int = 2048

@app.post('/generate')
async def generate_text(request: ProjectRequest):

    print(f'API HIT!!')

    try:

        # Define prompt template
        doc_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
        ### Instruction:
        {}
        ### Input:
        {}
        ### Response:
        """

        # Create a prompt asking for comprehensive documentation
        instruction = "Create a complete legal documentation section for a startup founder's report in the Information Technology sector, they provide Saas for a News Website"
        input_text = """The startup founders need a comprehensive legal documentation section covering:
        1. Founder's Agreement recommendations with equity allocation guidelines
        2. Intellectual Property Protection strategies and documentation
        3. Employee contracts and confidentiality agreements
        4. Terms of Service and Privacy Policy guidelines
        5. Regulatory compliance checklist specific to tech startups

        Make the document formal, thorough, and include section headers and sample clauses where appropriate. All answers should be in paragraph form with basic indentation only"""

        # Format the prompt
        prompt = doc_prompt.format(request.instruction, request.input_text)

        #data = prompt + ". Do all the above for this project: " + request.task

        if not prompt:
            return json.dumps({"error": "No prompt provided"}), 400

        # Tokenize and generate
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        #text_streamer = TextStreamer(tokenizer)

        # Generate comprehensive documentation
        print("Generating legal documentation section...\n")
        outputs = model.generate(
            **inputs,
            # streamer=text_streamer,
            max_new_tokens=1024,    # Allow for very long documentation
            temperature=0.7,        # Lower temperature for more formal, structured output
            top_p=0.92,             # Good diversity while staying professional
            repetition_penalty=1.2, # Prevent repetition
            do_sample=True,         # Enable sampling
            eos_token_id=None
        )

        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)


        print(f'Final Response : \n {type(output_text)}')
        final = {"generated_text": output_text}
        print(type(final))
        return final

    except Exception as e:
        print(f"Error occured bc : {e}")
        return json.dumps({"error": str(e)}), 500

from pyngrok import ngrok

# Set your ngrok authtoken
ngrok.set_auth_token("2tPtZ9OKATsdkx3rwhYmGMgBOOE_5RcvfHT8QQxdgDi3wTt2U")

public_url = ngrok.connect(8000).public_url
print(f"Public API URL: {public_url}")

# # Run the Flask app
# app.run(port=5000)

nest_asyncio.apply()

import uvicorn

public_url = ngrok.connect(8000).public_url
uvicorn.run(app, host="0.0.0.0", port =8000)