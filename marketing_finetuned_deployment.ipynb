{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gncoeEM3h3nY"
      },
      "outputs": [],
      "source": [
        "# Unsloth Model API Deployment\n",
        "\n",
        "# Install required packages\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import uvicorn\n",
        "import threading\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, List, Dict, Any\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer"
      ],
      "metadata": {
        "id": "-mUq9jmVh50v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up ngrok auth token (you'll need to sign up for ngrok and get a token)\n",
        "# You can skip this if you already set it up before\n",
        "import os\n",
        "NGROK_AUTH_TOKEN = \"\"  # Replace with your actual token\n",
        "!ngrok authtoken $NGROK_AUTH_TOKEN\n",
        "\n",
        "# Define your model parameters\n",
        "max_seq_length = 2048\n",
        "dtype = None  # None for auto detection (Float16 for Tesla T4, Bfloat16 for Ampere+)\n",
        "load_in_4bit = True\n",
        "\n",
        "# Replace with your actual model path on Hugging Face\n",
        "HF_MODEL_PATH = \"Hamza-Mubashir/marketing_rafam97_finetuned\"  # Replace this!\n"
      ],
      "metadata": {
        "id": "IvmKNiqwh7f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"VentureForce Multi-Agent API\",\n",
        "              description=\"API for generating text using a finetuned Llama model\")\n",
        "\n",
        "# Configure CORS\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Define request and response models\n",
        "class GenerationRequest(BaseModel):\n",
        "    instruction: str\n",
        "    input_text: str = \"\"\n",
        "    max_new_tokens: int = 2048\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "\n",
        "class GenerationResponse(BaseModel):\n",
        "    generated_text: str\n",
        "\n",
        "# Global variables for model and tokenizer\n",
        "model = None\n",
        "tokenizer = None\n",
        "\n",
        "# Load model function\n",
        "def load_model():\n",
        "    global model, tokenizer\n",
        "    try:\n",
        "        print(\"Loading model from Hugging Face...\")\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=HF_MODEL_PATH,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "            # token=\"hf_...\",  # Uncomment and add your token if using a private model\n",
        "        )\n",
        "        FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "        print(\"Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Define the prompt template - use the same as during training\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Define API endpoints\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"VentureForce Multi-Agent API is running. Send POST requests to /generate endpoint.\"}\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerationResponse)\n",
        "async def generate_text(request: GenerationRequest):\n",
        "    global model, tokenizer\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Model not loaded yet. Please try again later.\")\n",
        "\n",
        "    try:\n",
        "        # Format the prompt\n",
        "        formatted_prompt = alpaca_prompt.format(\n",
        "            request.instruction,\n",
        "            request.input_text,\n",
        "            \"\"  # Leave output blank for generation\n",
        "        )\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_new_tokens,\n",
        "                temperature=request.temperature,\n",
        "                top_p=request.top_p,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        full_output = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "        # Extract only the response part\n",
        "        response_prefix = \"### Response:\"\n",
        "        if response_prefix in full_output:\n",
        "            generated_text = full_output.split(response_prefix)[1].strip()\n",
        "        else:\n",
        "            generated_text = full_output\n",
        "\n",
        "        return {\"generated_text\": generated_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation error: {str(e)}\")\n",
        "\n",
        "# Function to prevent Colab from disconnecting (keeps sending requests to maintain connection)\n",
        "def keep_alive():\n",
        "    import time\n",
        "    import requests\n",
        "    import IPython.display\n",
        "    from google.colab import output\n",
        "\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        output.eval_js(\"new Audio('https://dummy.mp3').play();\")\n",
        "        IPython.display.clear_output(wait=True)\n",
        "        print(\"Server is still running.\")"
      ],
      "metadata": {
        "id": "S8_BfpHKh95U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "TKOI6dyuh_3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "# Start the server with ngrok\n",
        "print(\"Loading model and starting server...\")\n",
        "load_model()\n",
        "\n",
        "# Test model with a sample generation before starting the server\n",
        "print(\"\\nTesting model with a sample prompt:\")\n",
        "test_instruction = \"Venture Force - A Multi Agent Framework for Early Age Startups\"\n",
        "test_input = \"\"\"Large Language Model (LLM) dialogue agents have unveiled unforeseen limitations in specific domains\n",
        "due to their generalized training data with typical problems i.e poor contextual parsing, lack of domain\n",
        "knowledge, factual inaccuracies, ethical dilemmas, bias propagation, and hallucinations.\"\"\"\n",
        "\n",
        "formatted_prompt = alpaca_prompt.format(test_instruction, test_input, \"\")\n",
        "inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "print(\"Generated sample:\")\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Start ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"\\nPublic URL: {ngrok_tunnel.public_url}\")\n",
        "print(f\"API Endpoint: {ngrok_tunnel.public_url}/generate\")\n",
        "print(\"\\nExample curl command:\")\n",
        "print(f'''curl -X 'POST' \\\\\n",
        "  '{ngrok_tunnel.public_url}/generate' \\\\\n",
        "  -H 'Content-Type: application/json' \\\\\n",
        "  -d '{{\n",
        "    \"instruction\": \"Venture Force - A Multi Agent Framework for Early Age Startups\",\n",
        "    \"input_text\": \"I want to create a startup that helps small businesses with AI-powered customer service\",\n",
        "    \"max_new_tokens\": 1024\n",
        "  }}'\n",
        "''')\n",
        "\n",
        "# Keep the server alive in a separate thread\n",
        "import threading\n",
        "threading.Thread(target=keep_alive, daemon=True).start()\n",
        "\n",
        "# Start uvicorn server (this will block until the server is stopped)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "id": "nppZU9AAiBNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}