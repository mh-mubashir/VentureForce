{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbvK8ATOB5Ql"
      },
      "source": [
        "# Combined API Deployment for Project Agent and Marketing Agent\n",
        "\n",
        "This notebook combines two separate APIs:\n",
        "1. Project Agent API - Using Hugging Face model to generate project-related insights\n",
        "2. Marketing Agent API - Using a fine-tuned model for marketing-related responses\n",
        "\n",
        "Both APIs will run on different endpoints but hosted on the same Google Colab instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBSk_yE_B5Qm"
      },
      "source": [
        "## Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFdX8xVhB5Qm",
        "outputId": "613793b4-bd32-4c86-835e-d85da9141ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.3.17)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.19)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for both APIs\n",
        "!pip install transformers huggingface_hub fastapi uvicorn pyngrok pydantic\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftptu3vqB5Qn"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BAW0czTB5Qn"
      },
      "outputs": [],
      "source": [
        "# Common imports\n",
        "import os\n",
        "import threading\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "import sqlite3\n",
        "import json\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import Dict, Optional, List, Any\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Apply nest_asyncio to allow running asyncio code in Jupyter\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z68LdiSPB5Qn"
      },
      "source": [
        "## Configure ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4jQYfHGB5Qn"
      },
      "outputs": [],
      "source": [
        "ngrok_auth_token = \"\"\n",
        "ngrok.set_auth_token(ngrok_auth_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68MVoU9xB5Qn"
      },
      "source": [
        "## Project Agent API Setup\n",
        "\n",
        "Instead of using Llama 3.2, we'll use a more reliable model to avoid the 'apply_qkv' attribute error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXC8LSHPB5Qn",
        "outputId": "02f3d986-61c6-4cd2-b6a3-e4b27473e7ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-4f2431cff627>:128: DeprecationWarning: \n",
            "        on_event is deprecated, use lifespan event handlers instead.\n",
            "\n",
            "        Read more about it in the\n",
            "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
            "        \n",
            "  @project_app.on_event(\"shutdown\")\n"
          ]
        }
      ],
      "source": [
        "# Project Agent Imports\n",
        "from huggingface_hub import login\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# Authenticate with Hugging Face for Project Agent\n",
        "PROJECT_HF_TOKEN = \"\"  # Replace with your actual token if needed\n",
        "login(token=PROJECT_HF_TOKEN)\n",
        "\n",
        "# Create FastAPI app for Project Agent\n",
        "project_app = FastAPI()\n",
        "\n",
        "# Setup SQLite Database\n",
        "db_name = \"project_responses.db\"\n",
        "conn = sqlite3.connect(db_name)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS project_responses (\n",
        "    section TEXT,\n",
        "    heading TEXT,\n",
        "    sub_question TEXT,\n",
        "    response TEXT,\n",
        "    PRIMARY KEY (section, sub_question)\n",
        ")\n",
        "''')\n",
        "\n",
        "# Define Project Agent questions and sub-questions\n",
        "questions_and_subquestions = {\n",
        "    \"Technical Architecture\": [\n",
        "        (\"High Level System Architecture\", \"What should be the high-level system architecture?\"),\n",
        "        (\"Backend and Frontend Technologies\", \"What backend and frontend technologies would be suitable?\"),\n",
        "    ],\n",
        "    \"Required Technical Tools & Stack\": [\n",
        "        (\"Programming Languages and Integrations\", \"What programming languages, cloud services, and third-party integrations are needed?\"),\n",
        "    ],\n",
        "    \"Engineering Team Structure\": [\n",
        "        (\"Required Engineers and Expertise\", \"What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?\"),\n",
        "        (\"Development Team Size\", \"How many developers would be needed at each stage?\"),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Project Agent request model\n",
        "class ProjectRequest(BaseModel):\n",
        "    task: str\n",
        "\n",
        "# Using Hugging Face Inference API instead of loading model locally to avoid errors\n",
        "def generate_text_with_hf_api(prompt, model_name=\"mistralai/Mistral-7B-Instruct-v0.2\"):\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {PROJECT_HF_TOKEN}\"}\n",
        "\n",
        "    # Format for instruction models\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 150,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"return_full_text\": False\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Retry mechanism\n",
        "    max_retries = 3\n",
        "    retry_delay = 5  # seconds\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        response = requests.post(API_URL, headers=headers, json=payload)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            try:\n",
        "                return response.json()[0][\"generated_text\"].strip()\n",
        "            except (KeyError, IndexError):\n",
        "                print(f\"Unexpected response format: {response.json()}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                    continue\n",
        "        elif response.status_code == 503 and \"loading\" in response.text.lower():\n",
        "            print(f\"Model is loading. Waiting {retry_delay} seconds before retry...\")\n",
        "            time.sleep(retry_delay)\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "                continue\n",
        "\n",
        "    # Fallback response if all retries fail\n",
        "    return \"Unable to generate response at this time. Please try again later.\"\n",
        "\n",
        "# Project Agent API endpoint\n",
        "@project_app.post(\"/generate\")\n",
        "async def generate_project_responses(request: ProjectRequest):\n",
        "    # Prepare the JSON response structure\n",
        "    response_json = {}\n",
        "\n",
        "    for section, sub_questions in questions_and_subquestions.items():\n",
        "        section_responses = {}\n",
        "\n",
        "        for heading, sub_question in sub_questions:\n",
        "            prompt = f\"\"\"\n",
        "            Based on the following project idea: {request.task}\n",
        "\n",
        "            Provide a concise, well-structured, and informative response to the following question in a single paragraph only (The response should be just a single paragraph and not bullet points etc):\n",
        "\n",
        "            **{sub_question}**\n",
        "\n",
        "            Limit your response to 200-250 characters. The response should focus on providing key insights and actionable recommendations. Avoid conversational phrases like \"Choose the best answer for your project\" or any unnecessary options. The goal is to provide a detailed but succinct summary that directly addresses the question in a factual, professional tone.\n",
        "            \"\"\"\n",
        "            print(f\"Asking about: {sub_question}...\")\n",
        "\n",
        "            # Generate the response using Hugging Face API\n",
        "            response = generate_text_with_hf_api(prompt)\n",
        "\n",
        "            # Add response to the section\n",
        "            section_responses[heading] = response\n",
        "\n",
        "            # Store response in SQLite database\n",
        "            cursor.execute(\"INSERT OR REPLACE INTO project_responses (section, heading, sub_question, response) VALUES (?, ?, ?, ?)\",\n",
        "                          (section, heading, sub_question, response))\n",
        "            conn.commit()\n",
        "\n",
        "        # Add section responses to the main response\n",
        "        response_json[section] = section_responses\n",
        "\n",
        "    return {\"generated_text\": response_json}\n",
        "\n",
        "# Handle database closing\n",
        "@project_app.on_event(\"shutdown\")\n",
        "async def project_shutdown():\n",
        "    conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S30KevCB5Qo"
      },
      "source": [
        "## Marketing Agent API Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm0Kxgg8B5Qo"
      },
      "outputs": [],
      "source": [
        "# Marketing Agent Imports\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Create FastAPI app for Marketing Agent\n",
        "marketing_app = FastAPI()\n",
        "\n",
        "# Configure CORS for Marketing Agent\n",
        "marketing_app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Define Marketing Agent models\n",
        "class GenerationRequest(BaseModel):\n",
        "    instruction: str\n",
        "    input_text: str = \"\"\n",
        "    max_new_tokens: int = 2048\n",
        "    temperature: float = 0.7\n",
        "    top_p: float = 0.9\n",
        "\n",
        "class GenerationResponse(BaseModel):\n",
        "    generated_text: str\n",
        "\n",
        "# Marketing Agent model parameters\n",
        "max_seq_length = 2048\n",
        "dtype = None  # None for auto detection\n",
        "load_in_4bit = True\n",
        "HF_MODEL_PATH = \"Hamza-Mubashir/marketing_rafam97_finetuned\"  # Replace with your model if needed\n",
        "\n",
        "# Global variables for Marketing Agent model and tokenizer\n",
        "marketing_model = None\n",
        "marketing_tokenizer = None\n",
        "\n",
        "# Define the prompt template for Marketing Agent\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Load Marketing Agent model function\n",
        "def load_marketing_model():\n",
        "    global marketing_model, marketing_tokenizer\n",
        "    try:\n",
        "        print(\"Loading model for Marketing Agent...\")\n",
        "        marketing_model, marketing_tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=HF_MODEL_PATH,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=dtype,\n",
        "            load_in_4bit=load_in_4bit,\n",
        "        )\n",
        "        FastLanguageModel.for_inference(marketing_model)  # Enable faster inference\n",
        "        print(\"Marketing Agent model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Marketing Agent model: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Marketing Agent API endpoints\n",
        "@marketing_app.get(\"/\")\n",
        "async def marketing_root():\n",
        "    return {\"message\": \"Marketing Agent API is running. Send POST requests to /generate endpoint.\"}\n",
        "\n",
        "@marketing_app.post(\"/generate\", response_model=GenerationResponse)\n",
        "async def generate_marketing_text(request: GenerationRequest):\n",
        "    global marketing_model, marketing_tokenizer\n",
        "\n",
        "    if marketing_model is None or marketing_tokenizer is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Marketing Agent model not loaded yet. Please try again later.\")\n",
        "\n",
        "    try:\n",
        "        # Format the prompt\n",
        "        formatted_prompt = alpaca_prompt.format(\n",
        "            request.instruction,\n",
        "            request.input_text,\n",
        "            \"\"  # Leave output blank for generation\n",
        "        )\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = marketing_tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate text\n",
        "        with torch.no_grad():\n",
        "            outputs = marketing_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_new_tokens,\n",
        "                temperature=request.temperature,\n",
        "                top_p=request.top_p,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        # Decode and return the generated text\n",
        "        full_output = marketing_tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "        # Extract only the response part\n",
        "        response_prefix = \"### Response:\"\n",
        "        if response_prefix in full_output:\n",
        "            generated_text = full_output.split(response_prefix)[1].strip()\n",
        "        else:\n",
        "            generated_text = full_output\n",
        "\n",
        "        return {\"generated_text\": generated_text}\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Generation error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05IpklBRB5Qo"
      },
      "source": [
        "## Combined FastAPI Application Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_UaV6rVB5Qo"
      },
      "outputs": [],
      "source": [
        "# Create a main FastAPI app to mount both APIs\n",
        "main_app = FastAPI(title=\"Multi-Agent API System\")\n",
        "\n",
        "# Mount both APIs\n",
        "main_app.mount(\"/project\", project_app)\n",
        "main_app.mount(\"/marketing\", marketing_app)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_wEdVn0B5Qo"
      },
      "source": [
        "## Function to Keep Colab Alive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UoqK4QkB5Qo"
      },
      "outputs": [],
      "source": [
        "# Function to prevent Colab from disconnecting\n",
        "def keep_alive():\n",
        "    import time\n",
        "    import IPython.display\n",
        "    from google.colab import output\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            time.sleep(60)\n",
        "            output.eval_js(\"new Audio('https://dummy.mp3').play();\")\n",
        "            IPython.display.clear_output(wait=True)\n",
        "            print(\"Server is still running.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Keep-alive error: {e}\")\n",
        "            # Continue even if there's an error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amhIxv2YB5Qo"
      },
      "source": [
        "## Test the Marketing Agent Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZPg5C1PB5Qp",
        "outputId": "87797e95-2f01-4422-f857-f5f5066e97a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for Marketing Agent...\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-18' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Marketing Agent model loaded successfully!\n",
            "\n",
            "Testing Marketing Agent model with a sample prompt:\n",
            "Generated sample:\n",
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Venture Force - A Multi Agent Framework for Early Age Startups\n",
            "\n",
            "### Input:\n",
            "Large Language Model (LLM) dialogue agents have unveiled unforeseen limitations in specific domains\n",
            "        due to their generalized training data with typical problems i.e poor contextual parsing, lack of domain\n",
            "        knowledge, factual inaccuracies, ethical dilemmas, bias propagation, and hallucinations.\n",
            "\n",
            "### Response:\n",
            "Develop a Venture Force - a multi-agent framework integrating AI, human experts, and user feedback to address\n",
            "        domain-specific challenges. Utilize a combination of LLMs, rule-based systems, and knowledge graphs to\n",
            "        provide accurate, context-specific, and up-to-date information. Incorporate user feedback loops and\n",
            "        human oversight to mitigate bias and hallucinations.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Load and test the Marketing Agent model\n",
        "def test_marketing_model():\n",
        "    try:\n",
        "        # Test model with a sample generation\n",
        "        print(\"\\nTesting Marketing Agent model with a sample prompt:\")\n",
        "        test_instruction = \"Venture Force - A Multi Agent Framework for Early Age Startups\"\n",
        "        test_input = \"\"\"Large Language Model (LLM) dialogue agents have unveiled unforeseen limitations in specific domains\n",
        "        due to their generalized training data with typical problems i.e poor contextual parsing, lack of domain\n",
        "        knowledge, factual inaccuracies, ethical dilemmas, bias propagation, and hallucinations.\"\"\"\n",
        "\n",
        "        formatted_prompt = alpaca_prompt.format(test_instruction, test_input, \"\")\n",
        "        inputs = marketing_tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "        text_streamer = TextStreamer(marketing_tokenizer)\n",
        "        print(\"Generated sample:\")\n",
        "        _ = marketing_model.generate(\n",
        "            **inputs,\n",
        "            streamer=text_streamer,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            use_cache=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing marketing model: {e}\")\n",
        "        print(\"Continuing with API setup despite testing error...\")\n",
        "\n",
        "# Try to load Marketing model with error handling\n",
        "try:\n",
        "    load_marketing_model()\n",
        "    test_marketing_model()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or testing marketing model: {e}\")\n",
        "    print(\"The Marketing API may not function correctly, but we'll continue setting up the server.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaGIby5B5Qp"
      },
      "source": [
        "## Run the Combined Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hzVECLuIB5Qp",
        "outputId": "f328edf6-c676-434d-fd76-4dc8f3f0c44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Public URL: https://68b9-35-198-236-96.ngrok-free.app\n",
            "Project Agent API Endpoint: https://68b9-35-198-236-96.ngrok-free.app/project/generate\n",
            "Marketing Agent API Endpoint: https://68b9-35-198-236-96.ngrok-free.app/marketing/generate\n",
            "\n",
            "Example curl commands:\n",
            "# Project Agent API\n",
            "    curl -X 'POST'       'https://68b9-35-198-236-96.ngrok-free.app/project/generate'       -H 'Content-Type: application/json'       -d '{\n",
            "        \"task\": \"Build an AI-powered customer relationship management system\"\n",
            "      }'\n",
            "    \n",
            "# Marketing Agent API\n",
            "    curl -X 'POST'       'https://68b9-35-198-236-96.ngrok-free.app/marketing/generate'       -H 'Content-Type: application/json'       -d '{\n",
            "        \"instruction\": \"Create a marketing strategy\",\n",
            "        \"input_text\": \"Our startup provides AI-powered customer service solutions for small businesses\",\n",
            "        \"max_new_tokens\": 1024\n",
            "      }'\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [225]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keep-alive error: NotSupportedError: Failed to load because no supported source was found.\n",
            "INFO:     2601:19b:67c:83f0:cdfc:c2ef:8119:1957:0 - \"GET /project/generate HTTP/1.1\" 405 Method Not Allowed\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.169.116.93:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.169.116.93:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.105.6.46:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.105.6.46:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n",
            "Asking about: What should be the high-level system architecture?...\n",
            "Asking about: What backend and frontend technologies would be suitable?...\n",
            "Asking about: What programming languages, cloud services, and third-party integrations are needed?...\n",
            "Asking about: What kind of engineers and expertise are required (e.g., backend, frontend, DevOps, ML engineers, etc.)?...\n",
            "Asking about: How many developers would be needed at each stage?...\n",
            "INFO:     34.105.6.46:0 - \"POST /project/generate HTTP/1.1\" 200 OK\n",
            "INFO:     34.105.6.46:0 - \"POST /marketing/generate HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "# Start ngrok with error handling\n",
        "try:\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print(f\"\\nPublic URL: {ngrok_tunnel.public_url}\")\n",
        "    print(f\"Project Agent API Endpoint: {ngrok_tunnel.public_url}/project/generate\")\n",
        "    print(f\"Marketing Agent API Endpoint: {ngrok_tunnel.public_url}/marketing/generate\")\n",
        "\n",
        "    # Print example curl commands\n",
        "    print(\"\\nExample curl commands:\")\n",
        "    print(f'''# Project Agent API\n",
        "    curl -X 'POST' \\\n",
        "      '{ngrok_tunnel.public_url}/project/generate' \\\n",
        "      -H 'Content-Type: application/json' \\\n",
        "      -d '{{\n",
        "        \"task\": \"Build an AI-powered customer relationship management system\"\n",
        "      }}'\n",
        "    ''')\n",
        "\n",
        "    print(f'''# Marketing Agent API\n",
        "    curl -X 'POST' \\\n",
        "      '{ngrok_tunnel.public_url}/marketing/generate' \\\n",
        "      -H 'Content-Type: application/json' \\\n",
        "      -d '{{\n",
        "        \"instruction\": \"Create a marketing strategy\",\n",
        "        \"input_text\": \"Our startup provides AI-powered customer service solutions for small businesses\",\n",
        "        \"max_new_tokens\": 1024\n",
        "      }}'\n",
        "    '''\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up ngrok: {e}\")\n",
        "    print(\"Will attempt to start server without ngrok...\")\n",
        "\n",
        "# Keep the server alive in a separate thread with error handling\n",
        "thread = threading.Thread(target=keep_alive, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Start uvicorn server\n",
        "try:\n",
        "    uvicorn.run(main_app, host=\"0.0.0.0\", port=8000)\n",
        "except Exception as e:\n",
        "    print(f\"Error starting server: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
